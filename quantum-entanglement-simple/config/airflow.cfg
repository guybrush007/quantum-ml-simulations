[core]

donot_pickle = True

# Variable: AIRFLOW__SCHEDULER__PRINT_STATS_INTERVAL
#
print_stats_interval = 30

# How often (in seconds) should pool usage stats be sent to StatsD (if statsd_on is enabled)
#
# Variable: AIRFLOW__SCHEDULER__POOL_METRICS_INTERVAL
#
pool_metrics_interval = 5.0

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold
# ago (in seconds), scheduler is considered unhealthy.
# This is used by the health check in the "/health" endpoint and in `airflow jobs check` CLI
# for SchedulerJob.
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_THRESHOLD
#
scheduler_health_check_threshold = 30

# When you start a scheduler, airflow starts a tiny web server
# subprocess to serve a health check if this is set to True
#
# Variable: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
#
enable_health_check = False

# When you start a scheduler, airflow starts a tiny web server
# subprocess to serve a health check on this host
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_SERVER_HOST
#
scheduler_health_check_server_host = 0.0.0.0

# When you start a scheduler, airflow starts a tiny web server
# subprocess to serve a health check on this port
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_HEALTH_CHECK_SERVER_PORT
#
scheduler_health_check_server_port = 8974

# How often (in seconds) should the scheduler check for orphaned tasks and SchedulerJobs
#
# Variable: AIRFLOW__SCHEDULER__ORPHANED_TASKS_CHECK_INTERVAL
#
orphaned_tasks_check_interval = 300.0

# Determines the directory where logs for the child processes of the scheduler will be stored
#
# Variable: AIRFLOW__SCHEDULER__CHILD_PROCESS_LOG_DIRECTORY
#
child_process_log_directory = /opt/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULER_ZOMBIE_TASK_THRESHOLD
#
scheduler_zombie_task_threshold = 300

# How often (in seconds) should the scheduler check for zombie tasks.
#
# Variable: AIRFLOW__SCHEDULER__ZOMBIE_DETECTION_INTERVAL
#
zombie_detection_interval = 10.0

# Turn off scheduler catchup by setting this to ``False``.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is ``False``,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
#
# Variable: AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT
#
catchup_by_default = True

# Setting this to True will make first task instance of a task
# ignore depends_on_past setting. A task instance will be considered
# as the first task instance of a task when there is no task instance
# in the DB with an execution_date earlier than it., i.e. no manual marking
# success will be needed for a newly added task to be scheduled.
#
# Variable: AIRFLOW__SCHEDULER__IGNORE_FIRST_DEPENDS_ON_PAST_BY_DEFAULT
#
ignore_first_depends_on_past_by_default = True

# This changes the batch size of queries in the scheduling main loop.
# This should not be greater than ``core.parallelism``.
# If this is too high, SQL query performance may be impacted by
# complexity of query predicate, and/or excessive locking.
# Additionally, you may hit the maximum allowable query length for your db.
# Set this to 0 to use the value of ``core.parallelism``
#
# Variable: AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY
#
max_tis_per_query = 16

# Should the scheduler issue ``SELECT ... FOR UPDATE`` in relevant queries.
# If this is set to False then you should not run more than a single
# scheduler at once
#
# Variable: AIRFLOW__SCHEDULER__USE_ROW_LEVEL_LOCKING
#
use_row_level_locking = True

# Max number of DAGs to create DagRuns for per scheduler loop.
#
# Variable: AIRFLOW__SCHEDULER__MAX_DAGRUNS_TO_CREATE_PER_LOOP
#
max_dagruns_to_create_per_loop = 10

# How many DagRuns should a scheduler examine (and lock) when scheduling
# and queuing tasks.
#
# Variable: AIRFLOW__SCHEDULER__MAX_DAGRUNS_PER_LOOP_TO_SCHEDULE
#
max_dagruns_per_loop_to_schedule = 20

# Should the Task supervisor process perform a "mini scheduler" to attempt to schedule more tasks of the
# same DAG. Leaving this on will mean tasks in the same DAG execute quicker, but might starve out other
# dags in some circumstances
#
# Variable: AIRFLOW__SCHEDULER__SCHEDULE_AFTER_TASK_EXECUTION
#
schedule_after_task_execution = True

# The scheduler reads dag files to extract the airflow modules that are going to be used,
# and imports them ahead of time to avoid having to re-do it for each parsing process.
# This flag can be set to False to disable this behavior in case an airflow module needs to be freshly
# imported each time (at the cost of increased DAG parsing time).
#
# Variable: AIRFLOW__SCHEDULER__PARSING_PRE_IMPORT_MODULES
#
parsing_pre_import_modules = True

# The scheduler can run multiple processes in parallel to parse dags.
# This defines how many processes will run.
#
# Variable: AIRFLOW__SCHEDULER__PARSING_PROCESSES
#
parsing_processes = 2

# One of ``modified_time``, ``random_seeded_by_host`` and ``alphabetical``.
# The scheduler will list and sort the dag files to decide the parsing order.
# 
# * ``modified_time``: Sort by modified time of the files. This is useful on large scale to parse the
#   recently modified DAGs first.
# * ``random_seeded_by_host``: Sort randomly across multiple Schedulers but with same order on the
#   same host. This is useful when running with Scheduler in HA mode where each scheduler can
#   parse different DAG files.
# * ``alphabetical``: Sort by filename
#
# Variable: AIRFLOW__SCHEDULER__FILE_PARSING_SORT_MODE
#
file_parsing_sort_mode = modified_time

# Whether the dag processor is running as a standalone process or it is a subprocess of a scheduler
# job.
#
# Variable: AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR
#
standalone_dag_processor = False

# Only applicable if `[scheduler]standalone_dag_processor` is true and  callbacks are stored
# in database. Contains maximum number of callbacks that are fetched during a single loop.
#
# Variable: AIRFLOW__SCHEDULER__MAX_CALLBACKS_PER_LOOP
#
max_callbacks_per_loop = 20

# Only applicable if `[scheduler]standalone_dag_processor` is true.
# Time in seconds after which dags, which were not updated by Dag Processor are deactivated.
#
# Variable: AIRFLOW__SCHEDULER__DAG_STALE_NOT_SEEN_DURATION
#
dag_stale_not_seen_duration = 600

# Turn off scheduler use of cron intervals by setting this to False.
# DAGs submitted manually in the web UI or with trigger_dag will still run.
#
# Variable: AIRFLOW__SCHEDULER__USE_JOB_SCHEDULE
#
use_job_schedule = True

# Allow externally triggered DagRuns for Execution Dates in the future
# Only has effect if schedule_interval is set to None in DAG
#
# Variable: AIRFLOW__SCHEDULER__ALLOW_TRIGGER_IN_FUTURE
#
allow_trigger_in_future = False

# How often to check for expired trigger requests that have not run yet.
#
# Variable: AIRFLOW__SCHEDULER__TRIGGER_TIMEOUT_CHECK_INTERVAL
#
trigger_timeout_check_interval = 15

# Amount of time a task can be in the queued state before being retried or set to failed.
#
# Variable: AIRFLOW__SCHEDULER__TASK_QUEUED_TIMEOUT
#
task_queued_timeout = 600.0

# How often to check for tasks that have been in the queued state for
# longer than `[scheduler] task_queued_timeout`.
#
# Variable: AIRFLOW__SCHEDULER__TASK_QUEUED_TIMEOUT_CHECK_INTERVAL
#
task_queued_timeout_check_interval = 120.0

# The run_id pattern used to verify the validity of user input to the run_id parameter when
# triggering a DAG. This pattern cannot change the pattern used by scheduler to generate run_id
# for scheduled DAG runs or DAG runs triggered without changing the run_id parameter.
#
# Variable: AIRFLOW__SCHEDULER__ALLOWED_RUN_ID_PATTERN
#
allowed_run_id_pattern = ^[A-Za-z0-9_.~:+-]+$

# Whether to create DAG runs that span an interval or one single point in time for cron schedules, when
# a cron string is provided to `schedule` argument of a DAG. If True,
# CronDataIntervalTimetable is used, which is the legacy Airflow behavior suitable
# for DAGs with well-defined data_interval you get contiguous intervals from the end of the previous
# interval up to the scheduled datetime. If False, CronTriggerTimetable is used,
# which is closer to the behavior of cron itself.
# 
# Notably, for CronTriggerTimetable, the logical_date is the same as the time the DAG Run will try to
# schedule, while for CronDataIntervalTimetable, the logical_date is the beginning of the data interval,
# but the DAG Run will try to schedule at the end of the data interval. For more differences
# between the two Timetables, see
# https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/timetable.html#differences-between-the-two-cron-timetables
#
# Variable: AIRFLOW__SCHEDULER__CREATE_CRON_DATA_INTERVALS
#
create_cron_data_intervals = True

[triggerer]
# How many triggers a single Triggerer will run at once, by default.
#
# Variable: AIRFLOW__TRIGGERER__DEFAULT_CAPACITY
#
default_capacity = 1000

# How often to heartbeat the Triggerer job to ensure it hasn't been killed.
#
# Variable: AIRFLOW__TRIGGERER__JOB_HEARTBEAT_SEC
#
job_heartbeat_sec = 5

# If the last triggerer heartbeat happened more than triggerer_health_check_threshold
# ago (in seconds), triggerer is considered unhealthy.
# This is used by the health check in the "/health" endpoint and in `airflow jobs check` CLI
# for TriggererJob.
#
# Variable: AIRFLOW__TRIGGERER__TRIGGERER_HEALTH_CHECK_THRESHOLD
#
triggerer_health_check_threshold = 30

[kerberos]
# Location of your ccache file once kinit has been performed.
#
# Variable: AIRFLOW__KERBEROS__CCACHE
#
ccache = /tmp/airflow_krb5_ccache

# gets augmented with fqdn
#
# Variable: AIRFLOW__KERBEROS__PRINCIPAL
#
principal = airflow

# Determines the frequency at which initialization or re-initialization processes occur.
#
# Variable: AIRFLOW__KERBEROS__REINIT_FREQUENCY
#
reinit_frequency = 3600

# Path to the kinit executable
#
# Variable: AIRFLOW__KERBEROS__KINIT_PATH
#
kinit_path = kinit

# Designates the path to the Kerberos keytab file for the Airflow user
#
# Variable: AIRFLOW__KERBEROS__KEYTAB
#
keytab = airflow.keytab

# Allow to disable ticket forwardability.
#
# Variable: AIRFLOW__KERBEROS__FORWARDABLE
#
forwardable = True

# Allow to remove source IP from token, useful when using token behind NATted Docker host.
#
# Variable: AIRFLOW__KERBEROS__INCLUDE_IP
#
include_ip = True

[sensors]
# Sensor default timeout, 7 days by default (7 * 24 * 60 * 60).
#
# Variable: AIRFLOW__SENSORS__DEFAULT_TIMEOUT
#
default_timeout = 604800

[aws]
# This section contains settings for Amazon Web Services (AWS) integration.

# session_factory = 
cloudwatch_task_handler_json_serializer = airflow.providers.amazon.aws.log.cloudwatch_task_handler.json_serialize_legacy

[aws_batch_executor]
# This section only applies if you are using the AwsBatchExecutor in
# Airflow's ``[core]`` configuration.
# For more information on any of these execution parameters, see the link below:
# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/batch.html#Batch.Client.submit_job
# For boto3 credential management, see
# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html

conn_id = aws_default
# region_name = 
max_submit_job_attempts = 3
check_health_on_startup = True
# job_name = 
# job_queue = 
# job_definition = 
# submit_job_kwargs = 

[aws_ecs_executor]
# This section only applies if you are using the AwsEcsExecutor in
# Airflow's ``[core]`` configuration.
# For more information on any of these execution parameters, see the link below:
# https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs/client/run_task.html
# For boto3 credential management, see
# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html

conn_id = aws_default
# region_name = 
assign_public_ip = False
# cluster = 
# capacity_provider_strategy = 
# container_name = 
# launch_type = 
platform_version = LATEST
# security_groups = 
# subnets = 
# task_definition = 
max_run_task_attempts = 3
# run_task_kwargs = 
check_health_on_startup = True

[aws_auth_manager]
# This section only applies if you are using the AwsAuthManager. In other words, if you set
# ``[core] auth_manager = airflow.providers.amazon.aws.auth_manager.aws_auth_manager.AwsAuthManager`` in
# Airflow's configuration.

enable = False
conn_id = aws_default
# region_name = 
# saml_metadata_url = 
# avp_policy_store_id = 

[celery_kubernetes_executor]
# This section only applies if you are using the ``CeleryKubernetesExecutor`` in
# ``[core]`` section above

# Define when to send a task to ``KubernetesExecutor`` when using ``CeleryKubernetesExecutor``.
# When the queue of a task is the value of ``kubernetes_queue`` (default ``kubernetes``),
# the task is executed via ``KubernetesExecutor``,
# otherwise via ``CeleryExecutor``
#
# Variable: AIRFLOW__CELERY_KUBERNETES_EXECUTOR__KUBERNETES_QUEUE
#
kubernetes_queue = kubernetes

[celery]
# This section only applies if you are using the CeleryExecutor in
# ``[core]`` section above

# The app name that will be used by celery
#
# Variable: AIRFLOW__CELERY__CELERY_APP_NAME
#
celery_app_name = airflow.providers.celery.executors.celery_executor

# The concurrency that will be used when starting workers with the
# ``airflow celery worker`` command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
#
# Variable: AIRFLOW__CELERY__WORKER_CONCURRENCY
#
worker_concurrency = 16

# The maximum and minimum number of pool processes that will be used to dynamically resize
# the pool based on load.Enable autoscaling by providing max_concurrency,min_concurrency
# with the ``airflow celery worker`` command (always keep minimum processes,
# but grow to maximum if necessary).
# Pick these numbers based on resources on worker box and the nature of the task.
# If autoscale option is available, worker_concurrency will be ignored.
# https://docs.celeryq.dev/en/latest/reference/celery.bin.worker.html#cmdoption-celery-worker-autoscale
#
# Example: worker_autoscale = 16,12
#
# Variable: AIRFLOW__CELERY__WORKER_AUTOSCALE
#
# worker_autoscale = 

# Used to increase the number of tasks that a worker prefetches which can improve performance.
# The number of processes multiplied by worker_prefetch_multiplier is the number of tasks
# that are prefetched by a worker. A value greater than 1 can result in tasks being unnecessarily
# blocked if there are multiple workers and one worker prefetches tasks that sit behind long
# running tasks while another worker has unutilized processes that are unable to process the already
# claimed blocked tasks.
# https://docs.celeryq.dev/en/stable/userguide/optimizing.html#prefetch-limits
#
# Variable: AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER
#
worker_prefetch_multiplier = 1

# Specify if remote control of the workers is enabled.
# In some cases when the broker does not support remote control, Celery creates lots of
# ``.*reply-celery-pidbox`` queues. You can prevent this by setting this to false.
# However, with this disabled Flower won't work.
# https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/index.html#broker-overview
#
# Variable: AIRFLOW__CELERY__WORKER_ENABLE_REMOTE_CONTROL
#
worker_enable_remote_control = true

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more information.
#
# Variable: AIRFLOW__CELERY__BROKER_URL
#
broker_url = redis://redis:6379/0

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# When not specified, sql_alchemy_conn with a db+ scheme prefix will be used
# https://docs.celeryq.dev/en/latest/userguide/configuration.html#task-result-backend-settings
#
# Example: result_backend = db+postgresql://postgres:airflow@postgres/airflow
#
# Variable: AIRFLOW__CELERY__RESULT_BACKEND
#
# result_backend = 

# Optional configuration dictionary to pass to the Celery result backend SQLAlchemy engine.
#
# Example: result_backend_sqlalchemy_engine_options = {"pool_recycle": 1800}
#
# Variable: AIRFLOW__CELERY__RESULT_BACKEND_SQLALCHEMY_ENGINE_OPTIONS
#
result_backend_sqlalchemy_engine_options = 

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it ``airflow celery flower``. This defines the IP that Celery Flower runs on
#
# Variable: AIRFLOW__CELERY__FLOWER_HOST
#
flower_host = 0.0.0.0

# The root URL for Flower
#
# Example: flower_url_prefix = /flower
#
# Variable: AIRFLOW__CELERY__FLOWER_URL_PREFIX
#
flower_url_prefix = 

# This defines the port that Celery Flower runs on
#
# Variable: AIRFLOW__CELERY__FLOWER_PORT
#
flower_port = 5555

# Securing Flower with Basic Authentication
# Accepts user:password pairs separated by a comma
#
# Example: flower_basic_auth = user1:password1,user2:password2
#
# Variable: AIRFLOW__CELERY__FLOWER_BASIC_AUTH
#
flower_basic_auth = 

# How many processes CeleryExecutor uses to sync task state.
# 0 means to use max(1, number of cores - 1) processes.
#
# Variable: AIRFLOW__CELERY__SYNC_PARALLELISM
#
sync_parallelism = 0

# Import path for celery configuration options
#
# Variable: AIRFLOW__CELERY__CELERY_CONFIG_OPTIONS
#
celery_config_options = airflow.providers.celery.executors.default_celery.DEFAULT_CELERY_CONFIG

#
# Variable: AIRFLOW__CELERY__SSL_ACTIVE
#
ssl_active = False

# Path to the client key.
#
# Variable: AIRFLOW__CELERY__SSL_KEY
#
ssl_key = 

# Path to the client certificate.
#
# Variable: AIRFLOW__CELERY__SSL_CERT
#
ssl_cert = 

# Path to the CA certificate.
#
# Variable: AIRFLOW__CELERY__SSL_CACERT
#
ssl_cacert = 

# Celery Pool implementation.
# Choices include: ``prefork`` (default), ``eventlet``, ``gevent`` or ``solo``.
# See:
# https://docs.celeryq.dev/en/latest/userguide/workers.html#concurrency
# https://docs.celeryq.dev/en/latest/userguide/concurrency/eventlet.html
#
# Variable: AIRFLOW__CELERY__POOL
#
pool = prefork

# The number of seconds to wait before timing out ``send_task_to_executor`` or
# ``fetch_celery_task_state`` operations.
#
# Variable: AIRFLOW__CELERY__OPERATION_TIMEOUT
#
operation_timeout = 1.0

task_acks_late = True
# Celery task will report its status as 'started' when the task is executed by a worker.
# This is used in Airflow to keep track of the running tasks and if a Scheduler is restarted
# or run in HA mode, it can adopt the orphan tasks launched by previous SchedulerJob.
#
# Variable: AIRFLOW__CELERY__TASK_TRACK_STARTED
#
task_track_started = True

# The Maximum number of retries for publishing task messages to the broker when failing
# due to ``AirflowTaskTimeout`` error before giving up and marking Task as failed.
#
# Variable: AIRFLOW__CELERY__TASK_PUBLISH_MAX_RETRIES
#
task_publish_max_retries = 3

# Worker initialisation check to validate Metadata Database connection
#
# Variable: AIRFLOW__CELERY__WORKER_PRECHECK
#
worker_precheck = False

[celery_broker_transport_options]
# This section is for specifying options which can be passed to the
# underlying celery broker transport. See:
# https://docs.celeryq.dev/en/latest/userguide/configuration.html#std:setting-broker_transport_options

# The visibility timeout defines the number of seconds to wait for the worker
# to acknowledge the task before the message is redelivered to another worker.
# Make sure to increase the visibility timeout to match the time of the longest
# ETA you're planning to use.
# visibility_timeout is only supported for Redis and SQS celery brokers.
# See:
# https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/redis.html#visibility-timeout
#
# Example: visibility_timeout = 21600
#
# Variable: AIRFLOW__CELERY_BROKER_TRANSPORT_OPTIONS__VISIBILITY_TIMEOUT
#
# visibility_timeout = 

# The sentinel_kwargs parameter allows passing additional options to the Sentinel client.
# In a typical scenario where Redis Sentinel is used as the broker and Redis servers are
# password-protected, the password needs to be passed through this parameter. Although its
# type is string, it is required to pass a string that conforms to the dictionary format.
# See:
# https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/redis.html#configuration
#
# Example: sentinel_kwargs = {"password": "password_for_redis_server"}
#
# Variable: AIRFLOW__CELERY_BROKER_TRANSPORT_OPTIONS__SENTINEL_KWARGS
#
# sentinel_kwargs = 

[local_kubernetes_executor]
# This section only applies if you are using the ``LocalKubernetesExecutor`` in
# ``[core]`` section above

# Define when to send a task to ``KubernetesExecutor`` when using ``LocalKubernetesExecutor``.
# When the queue of a task is the value of ``kubernetes_queue`` (default ``kubernetes``),
# the task is executed via ``KubernetesExecutor``,
# otherwise via ``LocalExecutor``
#
# Variable: AIRFLOW__LOCAL_KUBERNETES_EXECUTOR__KUBERNETES_QUEUE
#
kubernetes_queue = kubernetes

[kubernetes_executor]
# Kwargs to override the default urllib3 Retry used in the kubernetes API client
#
# Example: api_client_retry_configuration = { "total": 3, "backoff_factor": 0.5 }
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__API_CLIENT_RETRY_CONFIGURATION
#
api_client_retry_configuration = 

# Flag to control the information added to kubernetes executor logs for better traceability
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__LOGS_TASK_METADATA
#
logs_task_metadata = False

# Path to the YAML pod file that forms the basis for KubernetesExecutor workers.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__POD_TEMPLATE_FILE
#
pod_template_file = 

# The repository of the Kubernetes Image for the Worker to Run
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_CONTAINER_REPOSITORY
#
worker_container_repository = 

# The tag of the Kubernetes Image for the Worker to Run
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_CONTAINER_TAG
#
worker_container_tag = 

# The Kubernetes namespace where airflow workers should be created. Defaults to ``default``
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__NAMESPACE
#
namespace = default

# If True, all worker pods will be deleted upon termination
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS
#
delete_worker_pods = True

# If False (and delete_worker_pods is True),
# failed worker pods will not be deleted so users can investigate them.
# This only prevents removal of worker pods where the worker itself failed,
# not when the task it ran failed.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_WORKER_PODS_ON_FAILURE
#
delete_worker_pods_on_failure = False

worker_pod_pending_fatal_container_state_reasons = CreateContainerConfigError,ErrImagePull,CreateContainerError,ImageInspectError, InvalidImageName
# Number of Kubernetes Worker Pod creation calls per scheduler loop.
# Note that the current default of "1" will only launch a single pod
# per-heartbeat. It is HIGHLY recommended that users increase this
# number to match the tolerance of their kubernetes cluster for
# better performance.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_PODS_CREATION_BATCH_SIZE
#
worker_pods_creation_batch_size = 1

# Allows users to launch pods in multiple namespaces.
# Will require creating a cluster-role for the scheduler,
# or use multi_namespace_mode_namespace_list configuration.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__MULTI_NAMESPACE_MODE
#
multi_namespace_mode = False

# If multi_namespace_mode is True while scheduler does not have a cluster-role,
# give the list of namespaces where the scheduler will schedule jobs
# Scheduler needs to have the necessary permissions in these namespaces.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__MULTI_NAMESPACE_MODE_NAMESPACE_LIST
#
multi_namespace_mode_namespace_list = 

# Use the service account kubernetes gives to pods to connect to kubernetes cluster.
# It's intended for clients that expect to be running inside a pod running on kubernetes.
# It will raise an exception if called from a process not running in a kubernetes environment.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__IN_CLUSTER
#
in_cluster = True

# When running with in_cluster=False change the default cluster_context or config_file
# options to Kubernetes client. Leave blank these to use default behaviour like ``kubectl`` has.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__CLUSTER_CONTEXT
#
# cluster_context = 

# Path to the kubernetes configfile to be used when ``in_cluster`` is set to False
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__CONFIG_FILE
#
# config_file = 

# Keyword parameters to pass while calling a kubernetes client core_v1_api methods
# from Kubernetes Executor provided as a single line formatted JSON dictionary string.
# List of supported params are similar for all core_v1_apis, hence a single config
# variable for all apis. See:
# https://raw.githubusercontent.com/kubernetes-client/python/41f11a09995efcd0142e25946adc7591431bfb2f/kubernetes/client/api/core_v1_api.py
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__KUBE_CLIENT_REQUEST_ARGS
#
kube_client_request_args = 

# Optional keyword arguments to pass to the ``delete_namespaced_pod`` kubernetes client
# ``core_v1_api`` method when using the Kubernetes Executor.
# This should be an object and can contain any of the options listed in the ``v1DeleteOptions``
# class defined here:
# https://github.com/kubernetes-client/python/blob/41f11a09995efcd0142e25946adc7591431bfb2f/kubernetes/client/models/v1_delete_options.py#L19
#
# Example: delete_option_kwargs = {"grace_period_seconds": 10}
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__DELETE_OPTION_KWARGS
#
delete_option_kwargs = 

# Enables TCP keepalive mechanism. This prevents Kubernetes API requests to hang indefinitely
# when idle connection is time-outed on services like cloud load balancers or firewalls.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__ENABLE_TCP_KEEPALIVE
#
enable_tcp_keepalive = True

# When the `enable_tcp_keepalive` option is enabled, TCP probes a connection that has
# been idle for `tcp_keep_idle` seconds.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__TCP_KEEP_IDLE
#
tcp_keep_idle = 120

# When the `enable_tcp_keepalive` option is enabled, if Kubernetes API does not respond
# to a keepalive probe, TCP retransmits the probe after `tcp_keep_intvl` seconds.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__TCP_KEEP_INTVL
#
tcp_keep_intvl = 30

# When the `enable_tcp_keepalive` option is enabled, if Kubernetes API does not respond
# to a keepalive probe, TCP retransmits the probe `tcp_keep_cnt number` of times before
# a connection is considered to be broken.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__TCP_KEEP_CNT
#
tcp_keep_cnt = 6

# Set this to false to skip verifying SSL certificate of Kubernetes python client.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__VERIFY_SSL
#
verify_ssl = True

# How often in seconds to check for task instances stuck in "queued" status without a pod
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__WORKER_PODS_QUEUED_CHECK_INTERVAL
#
worker_pods_queued_check_interval = 60

# Path to a CA certificate to be used by the Kubernetes client to verify the server's SSL certificate.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__SSL_CA_CERT
#
ssl_ca_cert = 

# The Maximum number of retries for queuing the task to the kubernetes scheduler when
# failing due to Kube API exceeded quota errors before giving up and marking task as failed.
# -1 for unlimited times.
#
# Variable: AIRFLOW__KUBERNETES_EXECUTOR__TASK_PUBLISH_MAX_RETRIES
#
task_publish_max_retries = 0

[common.io]
# Common IO configuration section

# Path to a location on object storage where XComs can be stored in url format.
#
# Example: xcom_objectstorage_path = s3://conn_id@bucket/path
#
# Variable: AIRFLOW__COMMON.IO__XCOM_OBJECTSTORAGE_PATH
#
xcom_objectstorage_path = 

# Threshold in bytes for storing XComs in object storage. -1 means always store in the
# database. 0 means always store in object storage. Any positive number means
# it will be stored in object storage if the size of the value is greater than the threshold.
#
# Example: xcom_objectstorage_threshold = 1000000
#
# Variable: AIRFLOW__COMMON.IO__XCOM_OBJECTSTORAGE_THRESHOLD
#
xcom_objectstorage_threshold = -1

# Compression algorithm to use when storing XComs in object storage. Supported algorithms
# are a.o.: snappy, zip, gzip, bz2, and lzma. If not specified, no compression will be used.
# Note that the compression algorithm must be available in the Python installation (e.g.
# python-snappy for snappy). Zip, gz, bz2 are available by default.
#
# Example: xcom_objectstorage_compression = gz
#
# Variable: AIRFLOW__COMMON.IO__XCOM_OBJECTSTORAGE_COMPRESSION
#
xcom_objectstorage_compression = 

[elasticsearch]
# Elasticsearch host
#
# Variable: AIRFLOW__ELASTICSEARCH__HOST
#
host = 

# Format of the log_id, which is used to query for a given tasks logs
#
# Variable: AIRFLOW__ELASTICSEARCH__LOG_ID_TEMPLATE
#
log_id_template = {dag_id}-{task_id}-{run_id}-{map_index}-{try_number}

# Used to mark the end of a log stream for a task
#
# Variable: AIRFLOW__ELASTICSEARCH__END_OF_LOG_MARK
#
end_of_log_mark = end_of_log

# Qualified URL for an elasticsearch frontend (like Kibana) with a template argument for log_id
# Code will construct log_id using the log_id template from the argument above.
# NOTE: scheme will default to https if one is not provided
#
# Example: frontend = http://localhost:5601/app/kibana#/discover?_a=(columns:!(message),query:(language:kuery,query:'log_id: "{log_id}"'),sort:!(log.offset,asc))
#
# Variable: AIRFLOW__ELASTICSEARCH__FRONTEND
#
frontend = 

# Write the task logs to the stdout of the worker, rather than the default files
#
# Variable: AIRFLOW__ELASTICSEARCH__WRITE_STDOUT
#
write_stdout = False

# Instead of the default log formatter, write the log lines as JSON
#
# Variable: AIRFLOW__ELASTICSEARCH__JSON_FORMAT
#
json_format = False

# Log fields to also attach to the json output, if enabled
#
# Variable: AIRFLOW__ELASTICSEARCH__JSON_FIELDS
#
json_fields = asctime, filename, lineno, levelname, message

# The field where host name is stored (normally either `host` or `host.name`)
#
# Variable: AIRFLOW__ELASTICSEARCH__HOST_FIELD
#
host_field = host

# The field where offset is stored (normally either `offset` or `log.offset`)
#
# Variable: AIRFLOW__ELASTICSEARCH__OFFSET_FIELD
#
offset_field = offset

# Comma separated list of index patterns to use when searching for logs (default: `_all`).
#
# Example: index_patterns = something-*
#
# Variable: AIRFLOW__ELASTICSEARCH__INDEX_PATTERNS
#
index_patterns = _all

[elasticsearch_configs]
#
# Variable: AIRFLOW__ELASTICSEARCH_CONFIGS__HTTP_COMPRESS
#
http_compress = False

#
# Variable: AIRFLOW__ELASTICSEARCH_CONFIGS__VERIFY_CERTS
#
verify_certs = True

[fab]
# This section contains configs specific to FAB provider.

# Boolean for enabling rate limiting on authentication endpoints.
#
# Variable: AIRFLOW__FAB__AUTH_RATE_LIMITED
#
auth_rate_limited = True

# Rate limit for authentication endpoints.
#
# Variable: AIRFLOW__FAB__AUTH_RATE_LIMIT
#
auth_rate_limit = 5 per 40 second

# Update FAB permissions and sync security manager roles
# on webserver startup
#
# Variable: AIRFLOW__FAB__UPDATE_FAB_PERMS
#
update_fab_perms = True

[imap]
# Options for IMAP provider.

# ssl_context = 

[azure_remote_logging]
# Configuration that needs to be set for enable remote logging in Azure Blob Storage

remote_wasb_log_container = airflow-logs

[openlineage]
# This section applies settings for OpenLineage integration.
# More about configuration and it's precedence can be found at
# https://airflow.apache.org/docs/apache-airflow-providers-openlineage/stable/guides/user.html#transport-setup

# Disable sending events without uninstalling the OpenLineage Provider by setting this to true.
#
# Variable: AIRFLOW__OPENLINEAGE__DISABLED
#
disabled = False

# Exclude some Operators from emitting OpenLineage events by passing a string of semicolon separated
# full import paths of Operators to disable.
#
# Example: disabled_for_operators = airflow.operators.bash.BashOperator;airflow.operators.python.PythonOperator
#
# Variable: AIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS
#
disabled_for_operators = 

# If this setting is enabled, OpenLineage integration won't collect and emit metadata,
# unless you explicitly enable it per `DAG` or `Task` using  `enable_lineage` method.
#
# Variable: AIRFLOW__OPENLINEAGE__SELECTIVE_ENABLE
#
selective_enable = False

# Set namespace that the lineage data belongs to, so that if you use multiple OpenLineage producers,
# events coming from them will be logically separated.
#
# Example: namespace = my_airflow_instance_1
#
# Variable: AIRFLOW__OPENLINEAGE__NAMESPACE
#
# namespace = 

# Register custom OpenLineage Extractors by passing a string of semicolon separated full import paths.
#
# Example: extractors = full.path.to.ExtractorClass;full.path.to.AnotherExtractorClass
#
# Variable: AIRFLOW__OPENLINEAGE__EXTRACTORS
#
# extractors = 

# Specify the path to the YAML configuration file.
# This ensures backwards compatibility with passing config through the `openlineage.yml` file.
#
# Example: config_path = full/path/to/openlineage.yml
#
# Variable: AIRFLOW__OPENLINEAGE__CONFIG_PATH
#
config_path = 

# Pass OpenLineage Client transport configuration as JSON string. It should contain type of the
# transport and additional options (different for each transport type). For more details see:
# https://openlineage.io/docs/client/python/#built-in-transport-types
# 
# Currently supported types are:
# 
#   * HTTP
#   * Kafka
#   * Console
#   * File
#
# Example: transport = {"type": "http", "url": "http://localhost:5000", "endpoint": "api/v1/lineage"}
#
# Variable: AIRFLOW__OPENLINEAGE__TRANSPORT
#
transport = 

# Disable the inclusion of source code in OpenLineage events by setting this to `true`.
# By default, several Operators (e.g. Python, Bash) will include their source code in the events
# unless disabled.
#
# Variable: AIRFLOW__OPENLINEAGE__DISABLE_SOURCE_CODE
#
# disable_source_code = 

[smtp_provider]
# Options for SMTP provider.

# ssl context to use when using SMTP and IMAP SSL connections. By default, the context is "default"
# which sets it to ``ssl.create_default_context()`` which provides the right balance between
# compatibility and security, it however requires that certificates in your operating system are
# updated and that SMTP/IMAP servers of yours have valid certificates that have corresponding public
# keys installed on your machines. You can switch it to "none" if you want to disable checking
# of the certificates, but it is not recommended as it allows MITM (man-in-the-middle) attacks
# if your infrastructure is not sufficiently secured. It should only be set temporarily while you
# are fixing your certificate configuration. This can be typically done by upgrading to newer
# version of the operating system you run Airflow components on,by upgrading/refreshing proper
# certificates in the OS or by updating certificates for your mail servers.
# 
# If you do not set this option explicitly, it will use Airflow "email.ssl_context" configuration,
# but if this configuration is not present, it will use "default" value.
#
# Example: ssl_context = default
#
# Variable: AIRFLOW__SMTP_PROVIDER__SSL_CONTEXT
#
# ssl_context = 

# Allows overriding of the standard templated email subject line when the SmtpNotifier is used.
# Must provide a path to the template.
#
# Example: templated_email_subject_path = path/to/override/email_subject.html
#
# Variable: AIRFLOW__SMTP_PROVIDER__TEMPLATED_EMAIL_SUBJECT_PATH
#
# templated_email_subject_path = 

# Allows overriding of the standard templated email path when the SmtpNotifier is used. Must provide
# a path to the template.
#
# Example: templated_html_content_path = path/to/override/email.html
#
# Variable: AIRFLOW__SMTP_PROVIDER__TEMPLATED_HTML_CONTENT_PATH
#
# templated_html_content_path = 